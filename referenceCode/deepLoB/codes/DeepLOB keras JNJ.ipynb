{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate - DeepLOB: Deep Convolutional Neural Networks for Limit Order Books\n",
    "(Paper authors: Zihao Zhang, Stefan Zohren, Stephen Roberts)\n",
    "\n",
    "Dataset source: https://drive.google.com/drive/folders/1Xen3aRid9ZZhFqJRgEMyETNazk02cNmv?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Conv2D, LeakyReLU, MaxPooling2D, concatenate, LSTM, Reshape, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import pandas_market_calendars as mcal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse = mcal.get_calendar('NYSE')\n",
    "dates = list(nyse.schedule(start_date='2020-01-01', end_date='2020-01-09').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_str_list = []\n",
    "for trading_day in dates:\n",
    "    dates_str_list.append(str(trading_day.date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_dict= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dates_str_list)):\n",
    "    date = dates_str_list[i]\n",
    "    if date not in daily_data_dict.keys():\n",
    "        date = dates_str_list[i]\n",
    "        daily_data_dict[date] = np.array(pd.read_csv('./data/JNJ_orderbook/JNJ_' + date + '_34200000_57600000_orderbook_10.csv',header = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_mean_dict = {}\n",
    "normalization_stddev_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,len(dates_str_list)):\n",
    "    date = dates_str_list[i]\n",
    "    \n",
    "    if (date not in normalization_mean_dict.keys()) or (date not in normalization_stddev_dict.keys()):\n",
    "        look_back_dates_list = dates_str_list[(i-5):i]\n",
    "        prev_5_day_orderbook_np = None\n",
    "        for look_back_date in look_back_dates_list:\n",
    "            if prev_5_day_orderbook_np is None:\n",
    "                prev_5_day_orderbook_np = daily_data_dict[look_back_date]\n",
    "            else:\n",
    "                prev_5_day_orderbook_np = np.vstack((prev_5_day_orderbook_np, daily_data_dict[look_back_date]))\n",
    "                \n",
    "        \n",
    "        price_mean = prev_5_day_orderbook_np[:,range(0,prev_5_day_orderbook_np.shape[1],2)].mean()\n",
    "        price_std = prev_5_day_orderbook_np[:,range(0,prev_5_day_orderbook_np.shape[1],2)].std()\n",
    "        size_mean = prev_5_day_orderbook_np[:,range(1,prev_5_day_orderbook_np.shape[1],2)].mean()\n",
    "        size_std = prev_5_day_orderbook_np[:,range(1,prev_5_day_orderbook_np.shape[1],2)].std()\n",
    "        \n",
    "        normalization_mean_dict[date] = np.repeat([[price_mean,size_mean]], 20, axis=0).flatten()\n",
    "        normalization_stddev_dict[date] = np.repeat([[price_std,size_std]], 20, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_norm_data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,len(dates_str_list)):\n",
    "    date = dates_str_list[i]\n",
    "    if date not in daily_norm_data_dict.keys():\n",
    "        daily_norm_data_dict[date] = (daily_data_dict[date] - normalization_mean_dict[date])/ normalization_stddev_dict[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159941, 40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_norm_data_dict['2020-01-09'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, k):\n",
    "    return np.convolve(x, np.ones(k), 'valid') / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(k, alpha, daily_data_dict):\n",
    "    daily_label_dict = {}\n",
    "    for date in list(daily_data_dict.keys())[5:]:\n",
    "        price_ask = daily_data_dict[date][:,0]\n",
    "        size_ask = daily_data_dict[date][:,1]\n",
    "        price_bid = daily_data_dict[date][:,2]\n",
    "        size_bid = daily_data_dict[date][:,3]\n",
    "        mid_price = (price_ask * size_bid + price_bid * size_ask) / (size_ask + size_bid)\n",
    "        future_k_avg_mid_price = moving_average(mid_price, k)[1:]\n",
    "        change_pct = (future_k_avg_mid_price - mid_price[:-k])/mid_price[:-k]\n",
    "        y_label = (-(change_pct < -alpha).astype(int))  + (change_pct > alpha).astype(int)\n",
    "        \n",
    "        daily_label_dict[date] = y_label.reshape(-1,1)\n",
    "    return daily_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=26\n",
    "date = '2020-01-09'\n",
    "price_ask = daily_data_dict[date][:,0]\n",
    "size_ask = daily_data_dict[date][:,1]\n",
    "price_bid = daily_data_dict[date][:,2]\n",
    "size_bid = daily_data_dict[date][:,3]\n",
    "mid_price = (price_ask * size_bid + price_bid * size_ask) / (size_ask + size_bid)\n",
    "future_k_avg_mid_price = moving_average(mid_price, k)[1:]\n",
    "change_pct = (future_k_avg_mid_price - mid_price[:-k])/mid_price[:-k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.970212233871668e-05"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(change_pct,33.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8990615131648153e-05"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(change_pct,66.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_y(k, alpha, timestamp_per_sample, daily_norm_data_dict, daily_data_dict):\n",
    "    #k is the number of future timesteps used to generate the label y\n",
    "    data_x = None\n",
    "    for date in daily_norm_data_dict.keys():\n",
    "        if data_x is None:\n",
    "            data_x = daily_norm_data_dict[date].copy()[:-k,:]\n",
    "        else:\n",
    "            data_x = np.vstack((data_x, daily_norm_data_dict[date][:-k,:]))\n",
    "    print(data_x.shape)\n",
    "    \n",
    "    daily_label_dict = generate_labels(k, alpha, daily_data_dict)\n",
    "    data_y = None\n",
    "    for date in daily_label_dict.keys():\n",
    "        if data_y is None:\n",
    "            data_y = daily_label_dict[date].copy()\n",
    "        else:\n",
    "            data_y = np.vstack((data_y, daily_label_dict[date]))\n",
    "            \n",
    "    [N, P_x] = data_x.shape\n",
    "#     P_y = data_y.shape[1]\n",
    "    \n",
    "    x = np.zeros([(N-timestamp_per_sample+1), timestamp_per_sample, P_x])\n",
    "    \n",
    "    for i in range(N-timestamp_per_sample+1):\n",
    "        x[i] = data_x[i:(i+timestamp_per_sample), :]\n",
    "        \n",
    "    x = x.reshape(x.shape + (1,))\n",
    "    y = data_y[(timestamp_per_sample-1):]\n",
    "    y = np_utils.to_categorical(y, 3)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 8\n",
    "# alpha = 7e-6\n",
    "# price_ask = daily_data_dict['2020-01-09'][:,0]\n",
    "# size_ask = daily_data_dict['2020-01-09'][:,1]\n",
    "# price_bid = daily_data_dict['2020-01-09'][:,2]\n",
    "# size_bid = daily_data_dict['2020-01-09'][:,3]\n",
    "# mid_price = (price_ask * size_bid + price_bid * size_ask) / (size_ask + size_bid)\n",
    "# future_k_avg_mid_price = moving_average(mid_price, k)[1:]\n",
    "# change_pct = (future_k_avg_mid_price - mid_price[:-k])/mid_price[:-k]\n",
    "# y_label = (-(change_pct < -alpha).astype(int))  + (change_pct > alpha).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.percentile(change_pct,33.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.percentile(change_pct,66.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159933, 40)\n"
     ]
    }
   ],
   "source": [
    "X,y = generate_X_y(k=8, alpha=7e-6, timestamp_per_sample=100,\n",
    "                   daily_norm_data_dict= daily_norm_data_dict, \n",
    "                   daily_data_dict = daily_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127867, 100, 40, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127867, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31967, 100, 40, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31967, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X\n",
    "# del y\n",
    "# del X2\n",
    "# del y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159861, 40)\n"
     ]
    }
   ],
   "source": [
    "X2,y2 = generate_X_y(k=80, alpha=3.63e-5, timestamp_per_sample=100,\n",
    "                     daily_norm_data_dict= daily_norm_data_dict, \n",
    "                   daily_data_dict = daily_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159915, 40)\n"
     ]
    }
   ],
   "source": [
    "X3,y3 = generate_X_y(k=26, alpha=1.93e-5, timestamp_per_sample=100,\n",
    "                     daily_norm_data_dict= daily_norm_data_dict, \n",
    "                   daily_data_dict = daily_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3,y3,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypter Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_timestep = 100\n",
    "feature_num = 40\n",
    "\n",
    "#Conv param\n",
    "conv_filter_num = 16\n",
    "\n",
    "#Inception module param\n",
    "inception_num = 32\n",
    "\n",
    "#LSTM param\n",
    "LSTM_num = 64\n",
    "\n",
    "#Activation param\n",
    "leaky_relu_alpha = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical crossentropy loss\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "# ADAM is used\n",
    "learning_rate = 0.01\n",
    "adam_epsilon = 1\n",
    "optimizer = Adam(lr=learning_rate, epsilon=1)\n",
    "\n",
    "# accuracy is used for stopping training\n",
    "metrics = ['accuracy']\n",
    "\n",
    "#max epoch num is not specified in paper, use 120 because paper mentions training stops at about 100 epochs\n",
    "num_epoch = 10000\n",
    "#stop training when validation accuracy does not improve for 20 epochs\n",
    "stop_epoch_num = 20\n",
    "#mini-batch size 32 from paper\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception model\n",
    "\n",
    "From paper: In our case, we split the input into a small set of lowerdimensional representations by using 1 × 1 convolutions, transform the representations by a set of filters, here 3 × 1 and 5 × 1, and then merge the outputs. A max-pooling layer is used inside the Inception Module, with stride 1 and zero padding. “Inception@32” represents one module and indicates all convolutional layers have 32 filters in this module, and the approach is depicted schematically in Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics):\n",
    "    \n",
    "    input_tensor = Input(shape=(lookback_timestep, feature_num, 1))\n",
    "    \n",
    "    # Conv block1\n",
    "    print(input_tensor.shape)\n",
    "    conv_layer1 = Conv2D(conv_filter_num, (1,2), strides=(1, 2))(input_tensor)\n",
    "    print(conv_layer1.shape)\n",
    "    conv_layer1 =LeakyReLU(alpha=leaky_relu_alpha)(conv_layer1)\n",
    "    print(conv_layer1.shape)\n",
    "    conv_layer1 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer1)\n",
    "    conv_first1 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer1)\n",
    "    print(conv_layer1.shape)\n",
    "    conv_layer1 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer1)\n",
    "    conv_layer1 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer1)\n",
    "    print(conv_layer1.shape)\n",
    "\n",
    "    # Conv block2\n",
    "    conv_layer2 = Conv2D(conv_filter_num, (1,2), strides=(1, 2))(conv_layer1)\n",
    "    conv_layer2 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer2)\n",
    "    print(conv_layer2.shape)\n",
    "    conv_layer2 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer2)\n",
    "    conv_layer2 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer2)\n",
    "    print(conv_layer2.shape)\n",
    "    conv_layer2 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer2)\n",
    "    conv_layer2 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer2)\n",
    "    print(conv_layer2.shape)\n",
    "\n",
    "    # Conv block3\n",
    "    conv_layer3 = Conv2D(conv_filter_num, (1,10))(conv_layer2)\n",
    "    conv_layer3 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer3)\n",
    "    print(conv_layer3.shape)\n",
    "    conv_layer3 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer3)\n",
    "    conv_layer3 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer3)\n",
    "    print(conv_layer3.shape)\n",
    "    conv_layer3 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer3)\n",
    "    conv_layer3 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer3)\n",
    "    print(conv_layer3.shape)\n",
    "    \n",
    "    # Inception module\n",
    "    inception_module1 = Conv2D(inception_num, (1,1), padding='same')(conv_layer3)\n",
    "    inception_module1 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module1)\n",
    "    print(inception_module1.shape)\n",
    "    inception_module1 = Conv2D(inception_num, (3,1), padding='same')(inception_module1)\n",
    "    inception_module1 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module1)\n",
    "    print(inception_module1.shape)\n",
    "\n",
    "    inception_module2 = Conv2D(inception_num, (1,1), padding='same')(conv_layer3)\n",
    "    inception_module2 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module2)\n",
    "    print(inception_module2.shape)\n",
    "    inception_module2 = Conv2D(inception_num, (5,1), padding='same')(inception_module2)\n",
    "    inception_module2 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module2)\n",
    "    print(inception_module2.shape)\n",
    "\n",
    "    inception_module3 = MaxPooling2D((3,1), strides=(1,1), padding='same')(conv_layer3)\n",
    "    print(inception_module3.shape)\n",
    "    inception_module3 = Conv2D(inception_num, (1,1), padding='same')(inception_module3)\n",
    "    print(inception_module3.shape)\n",
    "    inception_module3 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module3)\n",
    "    print(inception_module3.shape)\n",
    "    \n",
    "    inception_module_final = concatenate([inception_module1, inception_module2, inception_module3], axis=3)\n",
    "    print(inception_module_final.shape)\n",
    "    inception_module_final = Reshape((inception_module_final.shape[1], inception_module_final.shape[3]))(inception_module_final)\n",
    "    print(inception_module_final.shape)\n",
    "\n",
    "    # LSTM\n",
    "    LSTM_output = LSTM(LSTM_num)(inception_module_final)\n",
    "    print(LSTM_output.shape)\n",
    "\n",
    "    # Fully Connected Layer with softmax activation function for output\n",
    "    model_output = Dense(3, activation='softmax')(LSTM_output)\n",
    "    print(model_output.shape)\n",
    "    \n",
    "    DeepLOB_model = Model(inputs=input_tensor, outputs= model_output)  \n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n",
    "    \n",
    "    DeepLOB_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return DeepLOB_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 40, 1)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 10, 16)\n",
      "(None, 100, 10, 16)\n",
      "(None, 100, 10, 16)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 96)\n",
      "(None, 100, 96)\n",
      "(None, 64)\n",
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "DeepLOB_model = initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 40, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 20, 16)  48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 100, 20, 16)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 100, 20, 16)  1040        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 100, 20, 16)  1040        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 20, 16)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 100, 10, 16)  528         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 100, 10, 16)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 10, 16)  1040        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 100, 10, 16)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 10, 16)  1040        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100, 10, 16)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 100, 1, 16)   2576        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 100, 1, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 100, 1, 16)   1040        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 100, 1, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 100, 1, 16)   1040        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 100, 1, 16)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 100, 1, 32)   544         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 100, 1, 32)   544         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 100, 1, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 100, 1, 16)   0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 100, 1, 32)   3104        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 1, 32)   5152        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 1, 32)   544         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 100, 1, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 100, 1, 32)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 100, 1, 32)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 1, 96)   0           leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "                                                                 leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100, 96)      0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           41216       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            195         lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 60,691\n",
      "Trainable params: 60,691\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DeepLOB_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "3996/3996 - 315s - loss: 1.0985 - accuracy: 0.3387 - val_loss: 1.0984 - val_accuracy: 0.3381\n",
      "Epoch 2/10000\n",
      "3996/3996 - 352s - loss: 1.0980 - accuracy: 0.3441 - val_loss: 1.0973 - val_accuracy: 0.3445\n",
      "Epoch 3/10000\n",
      "3996/3996 - 311s - loss: 1.0945 - accuracy: 0.3667 - val_loss: 1.0889 - val_accuracy: 0.3887\n",
      "Epoch 4/10000\n",
      "3996/3996 - 297s - loss: 1.0681 - accuracy: 0.4186 - val_loss: 1.0506 - val_accuracy: 0.4420\n",
      "Epoch 5/10000\n",
      "3996/3996 - 326s - loss: 1.0330 - accuracy: 0.4594 - val_loss: 1.0238 - val_accuracy: 0.4697\n",
      "Epoch 6/10000\n",
      "3996/3996 - 335s - loss: 1.0199 - accuracy: 0.4744 - val_loss: 1.0192 - val_accuracy: 0.4770\n",
      "Epoch 7/10000\n",
      "3996/3996 - 369s - loss: 1.0140 - accuracy: 0.4824 - val_loss: 1.0146 - val_accuracy: 0.4837\n",
      "Epoch 8/10000\n",
      "3996/3996 - 365s - loss: 1.0097 - accuracy: 0.4873 - val_loss: 1.0114 - val_accuracy: 0.4798\n",
      "Epoch 9/10000\n",
      "3996/3996 - 349s - loss: 1.0058 - accuracy: 0.4915 - val_loss: 1.0036 - val_accuracy: 0.4951\n",
      "Epoch 10/10000\n",
      "3996/3996 - 349s - loss: 1.0024 - accuracy: 0.4953 - val_loss: 1.0025 - val_accuracy: 0.4967\n",
      "Epoch 11/10000\n",
      "3996/3996 - 357s - loss: 0.9996 - accuracy: 0.4978 - val_loss: 1.0000 - val_accuracy: 0.4996\n",
      "Epoch 12/10000\n",
      "3996/3996 - 376s - loss: 0.9969 - accuracy: 0.4999 - val_loss: 0.9991 - val_accuracy: 0.4969\n",
      "Epoch 13/10000\n",
      "3996/3996 - 380s - loss: 0.9944 - accuracy: 0.5016 - val_loss: 0.9908 - val_accuracy: 0.5084\n",
      "Epoch 14/10000\n",
      "3996/3996 - 437s - loss: 0.9917 - accuracy: 0.5062 - val_loss: 0.9936 - val_accuracy: 0.5019\n",
      "Epoch 15/10000\n",
      "3996/3996 - 372s - loss: 0.9896 - accuracy: 0.5075 - val_loss: 0.9914 - val_accuracy: 0.5072\n",
      "Epoch 16/10000\n",
      "3996/3996 - 373s - loss: 0.9875 - accuracy: 0.5086 - val_loss: 0.9853 - val_accuracy: 0.5108\n",
      "Epoch 17/10000\n",
      "3996/3996 - 346s - loss: 0.9850 - accuracy: 0.5123 - val_loss: 0.9868 - val_accuracy: 0.5099\n",
      "Epoch 18/10000\n",
      "3996/3996 - 367s - loss: 0.9833 - accuracy: 0.5135 - val_loss: 0.9854 - val_accuracy: 0.5089\n",
      "Epoch 19/10000\n",
      "3996/3996 - 362s - loss: 0.9817 - accuracy: 0.5140 - val_loss: 0.9832 - val_accuracy: 0.5110\n",
      "Epoch 20/10000\n",
      "3996/3996 - 370s - loss: 0.9799 - accuracy: 0.5159 - val_loss: 0.9874 - val_accuracy: 0.5109\n",
      "Epoch 21/10000\n",
      "3996/3996 - 373s - loss: 0.9782 - accuracy: 0.5187 - val_loss: 0.9830 - val_accuracy: 0.5133\n",
      "Epoch 22/10000\n",
      "3996/3996 - 980s - loss: 0.9767 - accuracy: 0.5194 - val_loss: 0.9810 - val_accuracy: 0.5178\n",
      "Epoch 23/10000\n",
      "3996/3996 - 1030s - loss: 0.9753 - accuracy: 0.5213 - val_loss: 0.9789 - val_accuracy: 0.5165\n",
      "Epoch 24/10000\n",
      "3996/3996 - 730s - loss: 0.9733 - accuracy: 0.5222 - val_loss: 0.9858 - val_accuracy: 0.5107\n",
      "Epoch 25/10000\n",
      "3996/3996 - 371s - loss: 0.9714 - accuracy: 0.5251 - val_loss: 0.9773 - val_accuracy: 0.5198\n",
      "Epoch 26/10000\n",
      "3996/3996 - 377s - loss: 0.9698 - accuracy: 0.5260 - val_loss: 0.9732 - val_accuracy: 0.5255\n",
      "Epoch 27/10000\n",
      "3996/3996 - 362s - loss: 0.9679 - accuracy: 0.5273 - val_loss: 0.9782 - val_accuracy: 0.5165\n",
      "Epoch 28/10000\n",
      "3996/3996 - 712s - loss: 0.9663 - accuracy: 0.5284 - val_loss: 0.9729 - val_accuracy: 0.5245\n",
      "Epoch 29/10000\n",
      "3996/3996 - 1104s - loss: 0.9646 - accuracy: 0.5293 - val_loss: 0.9720 - val_accuracy: 0.5266\n",
      "Epoch 30/10000\n",
      "3996/3996 - 389s - loss: 0.9629 - accuracy: 0.5321 - val_loss: 0.9724 - val_accuracy: 0.5212\n",
      "Epoch 31/10000\n",
      "3996/3996 - 369s - loss: 0.9606 - accuracy: 0.5334 - val_loss: 0.9736 - val_accuracy: 0.5221\n",
      "Epoch 32/10000\n",
      "3996/3996 - 377s - loss: 0.9592 - accuracy: 0.5361 - val_loss: 0.9694 - val_accuracy: 0.5254\n",
      "Epoch 33/10000\n",
      "3996/3996 - 376s - loss: 0.9575 - accuracy: 0.5378 - val_loss: 0.9646 - val_accuracy: 0.5328\n",
      "Epoch 34/10000\n",
      "3996/3996 - 375s - loss: 0.9552 - accuracy: 0.5387 - val_loss: 0.9648 - val_accuracy: 0.5326\n",
      "Epoch 35/10000\n",
      "3996/3996 - 372s - loss: 0.9525 - accuracy: 0.5404 - val_loss: 0.9684 - val_accuracy: 0.5258\n",
      "Epoch 36/10000\n",
      "3996/3996 - 373s - loss: 0.9508 - accuracy: 0.5431 - val_loss: 0.9657 - val_accuracy: 0.5295\n",
      "Epoch 37/10000\n",
      "3996/3996 - 374s - loss: 0.9489 - accuracy: 0.5445 - val_loss: 0.9628 - val_accuracy: 0.5309\n",
      "Epoch 38/10000\n",
      "3996/3996 - 374s - loss: 0.9466 - accuracy: 0.5456 - val_loss: 0.9844 - val_accuracy: 0.5122\n",
      "Epoch 39/10000\n",
      "3996/3996 - 373s - loss: 0.9439 - accuracy: 0.5485 - val_loss: 0.9576 - val_accuracy: 0.5374\n",
      "Epoch 40/10000\n",
      "3996/3996 - 378s - loss: 0.9423 - accuracy: 0.5488 - val_loss: 0.9596 - val_accuracy: 0.5347\n",
      "Epoch 41/10000\n",
      "3996/3996 - 381s - loss: 0.9398 - accuracy: 0.5504 - val_loss: 0.9561 - val_accuracy: 0.5386\n",
      "Epoch 42/10000\n",
      "3996/3996 - 336s - loss: 0.9374 - accuracy: 0.5526 - val_loss: 0.9534 - val_accuracy: 0.5411\n",
      "Epoch 43/10000\n",
      "3996/3996 - 334s - loss: 0.9347 - accuracy: 0.5545 - val_loss: 0.9530 - val_accuracy: 0.5391\n",
      "Epoch 44/10000\n",
      "3996/3996 - 351s - loss: 0.9313 - accuracy: 0.5577 - val_loss: 0.9531 - val_accuracy: 0.5381\n",
      "Epoch 45/10000\n",
      "3996/3996 - 370s - loss: 0.9295 - accuracy: 0.5592 - val_loss: 0.9537 - val_accuracy: 0.5384\n",
      "Epoch 46/10000\n",
      "3996/3996 - 363s - loss: 0.9267 - accuracy: 0.5605 - val_loss: 0.9496 - val_accuracy: 0.5409\n",
      "Epoch 47/10000\n",
      "3996/3996 - 377s - loss: 0.9241 - accuracy: 0.5634 - val_loss: 0.9493 - val_accuracy: 0.5469\n",
      "Epoch 48/10000\n",
      "3996/3996 - 372s - loss: 0.9217 - accuracy: 0.5642 - val_loss: 0.9444 - val_accuracy: 0.5455\n",
      "Epoch 49/10000\n",
      "3996/3996 - 379s - loss: 0.9187 - accuracy: 0.5665 - val_loss: 0.9449 - val_accuracy: 0.5475\n",
      "Epoch 50/10000\n",
      "3996/3996 - 379s - loss: 0.9156 - accuracy: 0.5687 - val_loss: 0.9415 - val_accuracy: 0.5488\n",
      "Epoch 51/10000\n",
      "3996/3996 - 346s - loss: 0.9117 - accuracy: 0.5703 - val_loss: 0.9396 - val_accuracy: 0.5508\n",
      "Epoch 52/10000\n",
      "3996/3996 - 337s - loss: 0.9086 - accuracy: 0.5730 - val_loss: 0.9372 - val_accuracy: 0.5569\n",
      "Epoch 53/10000\n",
      "3996/3996 - 344s - loss: 0.9042 - accuracy: 0.5758 - val_loss: 0.9414 - val_accuracy: 0.5500\n",
      "Epoch 54/10000\n",
      "3996/3996 - 351s - loss: 0.9008 - accuracy: 0.5787 - val_loss: 0.9301 - val_accuracy: 0.5581\n",
      "Epoch 55/10000\n",
      "3996/3996 - 374s - loss: 0.8971 - accuracy: 0.5830 - val_loss: 0.9339 - val_accuracy: 0.5608\n",
      "Epoch 56/10000\n",
      "3996/3996 - 370s - loss: 0.8941 - accuracy: 0.5849 - val_loss: 0.9297 - val_accuracy: 0.5583\n",
      "Epoch 57/10000\n",
      "3996/3996 - 367s - loss: 0.8913 - accuracy: 0.5860 - val_loss: 0.9190 - val_accuracy: 0.5679\n",
      "Epoch 58/10000\n",
      "3996/3996 - 350s - loss: 0.8869 - accuracy: 0.5885 - val_loss: 0.9223 - val_accuracy: 0.5675\n",
      "Epoch 59/10000\n",
      "3996/3996 - 358s - loss: 0.8842 - accuracy: 0.5905 - val_loss: 0.9260 - val_accuracy: 0.5630\n",
      "Epoch 60/10000\n",
      "3996/3996 - 359s - loss: 0.8796 - accuracy: 0.5948 - val_loss: 0.9200 - val_accuracy: 0.5675\n",
      "Epoch 61/10000\n",
      "3996/3996 - 352s - loss: 0.8772 - accuracy: 0.5962 - val_loss: 0.9208 - val_accuracy: 0.5679\n",
      "Epoch 62/10000\n",
      "3996/3996 - 356s - loss: 0.8713 - accuracy: 0.5995 - val_loss: 0.9150 - val_accuracy: 0.5724\n",
      "Epoch 63/10000\n",
      "3996/3996 - 319s - loss: 0.8675 - accuracy: 0.6019 - val_loss: 0.9258 - val_accuracy: 0.5667\n",
      "Epoch 64/10000\n",
      "3996/3996 - 314s - loss: 0.8637 - accuracy: 0.6059 - val_loss: 0.9051 - val_accuracy: 0.5784\n",
      "Epoch 65/10000\n",
      "3996/3996 - 315s - loss: 0.8595 - accuracy: 0.6073 - val_loss: 0.8974 - val_accuracy: 0.5831\n",
      "Epoch 66/10000\n",
      "3996/3996 - 313s - loss: 0.8569 - accuracy: 0.6092 - val_loss: 0.9017 - val_accuracy: 0.5834\n",
      "Epoch 67/10000\n",
      "3996/3996 - 315s - loss: 0.8525 - accuracy: 0.6124 - val_loss: 0.9265 - val_accuracy: 0.5656\n",
      "Epoch 68/10000\n",
      "3996/3996 - 326s - loss: 0.8487 - accuracy: 0.6143 - val_loss: 0.9111 - val_accuracy: 0.5813\n",
      "Epoch 69/10000\n",
      "3996/3996 - 324s - loss: 0.8451 - accuracy: 0.6171 - val_loss: 0.9075 - val_accuracy: 0.5796\n",
      "Epoch 70/10000\n",
      "3996/3996 - 324s - loss: 0.8398 - accuracy: 0.6206 - val_loss: 0.9241 - val_accuracy: 0.5662\n",
      "Epoch 71/10000\n",
      "3996/3996 - 326s - loss: 0.8377 - accuracy: 0.6209 - val_loss: 0.8851 - val_accuracy: 0.5907\n",
      "Epoch 72/10000\n",
      "3996/3996 - 333s - loss: 0.8318 - accuracy: 0.6247 - val_loss: 0.9203 - val_accuracy: 0.5740\n",
      "Epoch 73/10000\n",
      "3996/3996 - 329s - loss: 0.8287 - accuracy: 0.6268 - val_loss: 0.8775 - val_accuracy: 0.5990\n",
      "Epoch 74/10000\n",
      "3996/3996 - 326s - loss: 0.8250 - accuracy: 0.6290 - val_loss: 0.8905 - val_accuracy: 0.5882\n",
      "Epoch 75/10000\n",
      "3996/3996 - 336s - loss: 0.8194 - accuracy: 0.6325 - val_loss: 0.8880 - val_accuracy: 0.5940\n",
      "Epoch 76/10000\n",
      "3996/3996 - 345s - loss: 0.8140 - accuracy: 0.6358 - val_loss: 0.8862 - val_accuracy: 0.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/10000\n",
      "3996/3996 - 332s - loss: 0.8113 - accuracy: 0.6377 - val_loss: 0.8903 - val_accuracy: 0.5929\n",
      "Epoch 78/10000\n",
      "3996/3996 - 326s - loss: 0.8088 - accuracy: 0.6401 - val_loss: 0.8831 - val_accuracy: 0.5955\n",
      "Epoch 79/10000\n",
      "3996/3996 - 324s - loss: 0.8020 - accuracy: 0.6434 - val_loss: 0.8900 - val_accuracy: 0.5936\n",
      "Epoch 80/10000\n",
      "3996/3996 - 326s - loss: 0.8004 - accuracy: 0.6432 - val_loss: 0.8783 - val_accuracy: 0.6022\n",
      "Epoch 81/10000\n",
      "3996/3996 - 328s - loss: 0.7949 - accuracy: 0.6481 - val_loss: 0.8923 - val_accuracy: 0.5955\n",
      "Epoch 82/10000\n",
      "3996/3996 - 335s - loss: 0.7921 - accuracy: 0.6502 - val_loss: 0.8768 - val_accuracy: 0.6055\n",
      "Epoch 83/10000\n",
      "3996/3996 - 329s - loss: 0.7881 - accuracy: 0.6523 - val_loss: 0.8684 - val_accuracy: 0.6067\n",
      "Epoch 84/10000\n",
      "3996/3996 - 327s - loss: 0.7837 - accuracy: 0.6538 - val_loss: 0.8756 - val_accuracy: 0.6045\n",
      "Epoch 85/10000\n",
      "3996/3996 - 328s - loss: 0.7792 - accuracy: 0.6568 - val_loss: 0.8888 - val_accuracy: 0.5974\n",
      "Epoch 86/10000\n",
      "3996/3996 - 360s - loss: 0.7759 - accuracy: 0.6576 - val_loss: 0.8590 - val_accuracy: 0.6165\n",
      "Epoch 87/10000\n",
      "3996/3996 - 369s - loss: 0.7686 - accuracy: 0.6621 - val_loss: 0.8476 - val_accuracy: 0.6216\n",
      "Epoch 88/10000\n",
      "3996/3996 - 362s - loss: 0.7693 - accuracy: 0.6625 - val_loss: 0.8548 - val_accuracy: 0.6179\n",
      "Epoch 89/10000\n",
      "3996/3996 - 330s - loss: 0.7641 - accuracy: 0.6667 - val_loss: 0.8588 - val_accuracy: 0.6167\n",
      "Epoch 90/10000\n",
      "3996/3996 - 329s - loss: 0.7629 - accuracy: 0.6677 - val_loss: 0.8656 - val_accuracy: 0.6144\n",
      "Epoch 91/10000\n",
      "3996/3996 - 352s - loss: 0.7552 - accuracy: 0.6710 - val_loss: 0.8663 - val_accuracy: 0.6079\n",
      "Epoch 92/10000\n",
      "3996/3996 - 346s - loss: 0.7526 - accuracy: 0.6715 - val_loss: 0.8723 - val_accuracy: 0.6079\n",
      "Epoch 93/10000\n",
      "3996/3996 - 346s - loss: 0.7500 - accuracy: 0.6727 - val_loss: 0.8648 - val_accuracy: 0.6149\n",
      "Epoch 94/10000\n",
      "3996/3996 - 346s - loss: 0.7467 - accuracy: 0.6745 - val_loss: 0.8720 - val_accuracy: 0.6092\n",
      "Epoch 95/10000\n",
      "3996/3996 - 338s - loss: 0.7417 - accuracy: 0.6778 - val_loss: 0.8753 - val_accuracy: 0.6067\n",
      "Epoch 96/10000\n",
      "3996/3996 - 359s - loss: 0.7348 - accuracy: 0.6819 - val_loss: 0.8387 - val_accuracy: 0.6269\n",
      "Epoch 97/10000\n",
      "3996/3996 - 356s - loss: 0.7348 - accuracy: 0.6814 - val_loss: 0.8338 - val_accuracy: 0.6310\n",
      "Epoch 98/10000\n",
      "3996/3996 - 369s - loss: 0.7307 - accuracy: 0.6846 - val_loss: 0.8503 - val_accuracy: 0.6240\n",
      "Epoch 99/10000\n",
      "3996/3996 - 361s - loss: 0.7257 - accuracy: 0.6873 - val_loss: 0.8352 - val_accuracy: 0.6310\n",
      "Epoch 100/10000\n",
      "3996/3996 - 368s - loss: 0.7189 - accuracy: 0.6916 - val_loss: 0.8307 - val_accuracy: 0.6360\n",
      "Epoch 101/10000\n",
      "3996/3996 - 365s - loss: 0.7198 - accuracy: 0.6910 - val_loss: 0.8419 - val_accuracy: 0.6301\n",
      "Epoch 102/10000\n",
      "3996/3996 - 367s - loss: 0.7180 - accuracy: 0.6923 - val_loss: 0.8247 - val_accuracy: 0.6392\n",
      "Epoch 103/10000\n",
      "3996/3996 - 363s - loss: 0.7169 - accuracy: 0.6922 - val_loss: 0.8255 - val_accuracy: 0.6385\n",
      "Epoch 104/10000\n",
      "3996/3996 - 368s - loss: 0.7095 - accuracy: 0.6961 - val_loss: 0.8357 - val_accuracy: 0.6363\n",
      "Epoch 105/10000\n",
      "3996/3996 - 375s - loss: 0.7057 - accuracy: 0.6977 - val_loss: 0.8180 - val_accuracy: 0.6393\n",
      "Epoch 106/10000\n",
      "3996/3996 - 374s - loss: 0.7019 - accuracy: 0.6992 - val_loss: 0.8318 - val_accuracy: 0.6366\n",
      "Epoch 107/10000\n",
      "3996/3996 - 367s - loss: 0.6977 - accuracy: 0.7022 - val_loss: 0.8155 - val_accuracy: 0.6410\n",
      "Epoch 108/10000\n",
      "3996/3996 - 650s - loss: 0.6996 - accuracy: 0.7023 - val_loss: 0.8185 - val_accuracy: 0.6418\n",
      "Epoch 109/10000\n",
      "3996/3996 - 934s - loss: 0.6926 - accuracy: 0.7047 - val_loss: 0.8382 - val_accuracy: 0.6327\n",
      "Epoch 110/10000\n",
      "3996/3996 - 398s - loss: 0.6916 - accuracy: 0.7054 - val_loss: 0.8058 - val_accuracy: 0.6509\n",
      "Epoch 111/10000\n",
      "3996/3996 - 346s - loss: 0.6854 - accuracy: 0.7093 - val_loss: 0.8309 - val_accuracy: 0.6426\n",
      "Epoch 112/10000\n",
      "3996/3996 - 341s - loss: 0.6829 - accuracy: 0.7108 - val_loss: 0.8440 - val_accuracy: 0.6371\n",
      "Epoch 113/10000\n",
      "3996/3996 - 531s - loss: 0.6824 - accuracy: 0.7097 - val_loss: 0.8164 - val_accuracy: 0.6482\n",
      "Epoch 114/10000\n",
      "3996/3996 - 773s - loss: 0.6782 - accuracy: 0.7123 - val_loss: 0.7949 - val_accuracy: 0.6540\n",
      "Epoch 115/10000\n",
      "3996/3996 - 342s - loss: 0.6741 - accuracy: 0.7141 - val_loss: 0.7992 - val_accuracy: 0.6583\n",
      "Epoch 116/10000\n",
      "3996/3996 - 341s - loss: 0.6734 - accuracy: 0.7154 - val_loss: 0.8138 - val_accuracy: 0.6459\n",
      "Epoch 117/10000\n",
      "3996/3996 - 356s - loss: 0.6686 - accuracy: 0.7151 - val_loss: 0.8253 - val_accuracy: 0.6426\n",
      "Epoch 118/10000\n",
      "3996/3996 - 345s - loss: 0.6655 - accuracy: 0.7193 - val_loss: 0.8151 - val_accuracy: 0.6518\n",
      "Epoch 119/10000\n",
      "3996/3996 - 354s - loss: 0.6628 - accuracy: 0.7207 - val_loss: 0.8169 - val_accuracy: 0.6479\n",
      "Epoch 120/10000\n",
      "3996/3996 - 350s - loss: 0.6630 - accuracy: 0.7204 - val_loss: 0.8152 - val_accuracy: 0.6448\n",
      "Epoch 121/10000\n",
      "3996/3996 - 341s - loss: 0.6608 - accuracy: 0.7217 - val_loss: 0.8189 - val_accuracy: 0.6502\n",
      "Epoch 122/10000\n",
      "3996/3996 - 367s - loss: 0.6536 - accuracy: 0.7256 - val_loss: 0.7989 - val_accuracy: 0.6625\n",
      "Epoch 123/10000\n",
      "3996/3996 - 377s - loss: 0.6517 - accuracy: 0.7272 - val_loss: 0.7929 - val_accuracy: 0.6592\n",
      "Epoch 124/10000\n",
      "3996/3996 - 383s - loss: 0.6549 - accuracy: 0.7234 - val_loss: 0.7846 - val_accuracy: 0.6646\n",
      "Epoch 125/10000\n",
      "3996/3996 - 374s - loss: 0.6481 - accuracy: 0.7271 - val_loss: 0.8435 - val_accuracy: 0.6360\n",
      "Epoch 126/10000\n",
      "3996/3996 - 365s - loss: 0.6445 - accuracy: 0.7299 - val_loss: 0.7787 - val_accuracy: 0.6670\n",
      "Epoch 127/10000\n",
      "3996/3996 - 380s - loss: 0.6434 - accuracy: 0.7293 - val_loss: 0.8245 - val_accuracy: 0.6405\n",
      "Epoch 128/10000\n",
      "3996/3996 - 372s - loss: 0.6424 - accuracy: 0.7302 - val_loss: 0.8148 - val_accuracy: 0.6506\n",
      "Epoch 129/10000\n",
      "3996/3996 - 359s - loss: 0.6373 - accuracy: 0.7329 - val_loss: 0.8133 - val_accuracy: 0.6518\n",
      "Epoch 130/10000\n",
      "3996/3996 - 391s - loss: 0.6388 - accuracy: 0.7333 - val_loss: 0.7965 - val_accuracy: 0.6607\n",
      "Epoch 131/10000\n",
      "3996/3996 - 337s - loss: 0.6409 - accuracy: 0.7318 - val_loss: 0.7892 - val_accuracy: 0.6636\n",
      "Epoch 132/10000\n",
      "3996/3996 - 298s - loss: 0.6341 - accuracy: 0.7349 - val_loss: 0.7922 - val_accuracy: 0.6691\n",
      "Epoch 133/10000\n",
      "3996/3996 - 296s - loss: 0.6304 - accuracy: 0.7364 - val_loss: 0.8010 - val_accuracy: 0.6596\n",
      "Epoch 134/10000\n",
      "3996/3996 - 295s - loss: 0.6253 - accuracy: 0.7378 - val_loss: 0.7688 - val_accuracy: 0.6759\n",
      "Epoch 135/10000\n",
      "3996/3996 - 295s - loss: 0.6235 - accuracy: 0.7392 - val_loss: 0.7741 - val_accuracy: 0.6741\n",
      "Epoch 136/10000\n",
      "3996/3996 - 296s - loss: 0.6233 - accuracy: 0.7398 - val_loss: 0.7619 - val_accuracy: 0.6787\n",
      "Epoch 137/10000\n",
      "3996/3996 - 295s - loss: 0.6236 - accuracy: 0.7411 - val_loss: 0.7964 - val_accuracy: 0.6670\n",
      "Epoch 138/10000\n",
      "3996/3996 - 294s - loss: 0.6181 - accuracy: 0.7436 - val_loss: 0.7771 - val_accuracy: 0.6700\n",
      "Epoch 139/10000\n",
      "3996/3996 - 296s - loss: 0.6205 - accuracy: 0.7418 - val_loss: 0.7737 - val_accuracy: 0.6725\n",
      "Epoch 140/10000\n",
      "3996/3996 - 294s - loss: 0.6113 - accuracy: 0.7463 - val_loss: 0.7817 - val_accuracy: 0.6699\n",
      "Epoch 141/10000\n",
      "3996/3996 - 294s - loss: 0.6134 - accuracy: 0.7448 - val_loss: 0.7852 - val_accuracy: 0.6739\n",
      "Epoch 142/10000\n",
      "3996/3996 - 296s - loss: 0.6133 - accuracy: 0.7453 - val_loss: 0.7526 - val_accuracy: 0.6829\n",
      "Epoch 143/10000\n",
      "3996/3996 - 294s - loss: 0.6107 - accuracy: 0.7473 - val_loss: 0.7639 - val_accuracy: 0.6795\n",
      "Epoch 144/10000\n",
      "3996/3996 - 293s - loss: 0.6071 - accuracy: 0.7478 - val_loss: 0.8044 - val_accuracy: 0.6629\n",
      "Epoch 145/10000\n",
      "3996/3996 - 292s - loss: 0.6074 - accuracy: 0.7485 - val_loss: 0.7947 - val_accuracy: 0.6671\n",
      "Epoch 146/10000\n",
      "3996/3996 - 292s - loss: 0.6040 - accuracy: 0.7498 - val_loss: 0.7879 - val_accuracy: 0.6692\n",
      "Epoch 147/10000\n",
      "3996/3996 - 291s - loss: 0.6029 - accuracy: 0.7483 - val_loss: 0.7564 - val_accuracy: 0.6802\n",
      "Epoch 148/10000\n",
      "3996/3996 - 296s - loss: 0.5978 - accuracy: 0.7523 - val_loss: 0.7688 - val_accuracy: 0.6790\n",
      "Epoch 149/10000\n",
      "3996/3996 - 307s - loss: 0.5982 - accuracy: 0.7528 - val_loss: 0.8013 - val_accuracy: 0.6685\n",
      "Epoch 150/10000\n",
      "3996/3996 - 302s - loss: 0.5954 - accuracy: 0.7535 - val_loss: 0.7713 - val_accuracy: 0.6767\n",
      "Epoch 151/10000\n",
      "3996/3996 - 304s - loss: 0.5922 - accuracy: 0.7553 - val_loss: 0.8350 - val_accuracy: 0.6505\n",
      "Epoch 152/10000\n",
      "3996/3996 - 303s - loss: 0.5954 - accuracy: 0.7530 - val_loss: 0.7580 - val_accuracy: 0.6862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/10000\n",
      "3996/3996 - 302s - loss: 0.5890 - accuracy: 0.7577 - val_loss: 0.8001 - val_accuracy: 0.6663\n",
      "Epoch 154/10000\n",
      "3996/3996 - 305s - loss: 0.5948 - accuracy: 0.7552 - val_loss: 0.7463 - val_accuracy: 0.6948\n",
      "Epoch 155/10000\n",
      "3996/3996 - 300s - loss: 0.5823 - accuracy: 0.7597 - val_loss: 0.7629 - val_accuracy: 0.6862\n",
      "Epoch 156/10000\n",
      "3996/3996 - 311s - loss: 0.5837 - accuracy: 0.7586 - val_loss: 0.7551 - val_accuracy: 0.6874\n",
      "Epoch 157/10000\n",
      "3996/3996 - 303s - loss: 0.5816 - accuracy: 0.7607 - val_loss: 0.7496 - val_accuracy: 0.6889\n",
      "Epoch 158/10000\n",
      "3996/3996 - 301s - loss: 0.5822 - accuracy: 0.7596 - val_loss: 0.7935 - val_accuracy: 0.6719\n",
      "Epoch 159/10000\n",
      "3996/3996 - 301s - loss: 0.5805 - accuracy: 0.7617 - val_loss: 0.7535 - val_accuracy: 0.6895\n",
      "Epoch 160/10000\n",
      "3996/3996 - 304s - loss: 0.5824 - accuracy: 0.7598 - val_loss: 0.7802 - val_accuracy: 0.6774\n",
      "Epoch 161/10000\n",
      "3996/3996 - 302s - loss: 0.5807 - accuracy: 0.7617 - val_loss: 0.7396 - val_accuracy: 0.6966\n",
      "Epoch 162/10000\n",
      "3996/3996 - 302s - loss: 0.5773 - accuracy: 0.7640 - val_loss: 0.7762 - val_accuracy: 0.6845\n",
      "Epoch 163/10000\n",
      "3996/3996 - 304s - loss: 0.5797 - accuracy: 0.7618 - val_loss: 0.7568 - val_accuracy: 0.6865\n",
      "Epoch 164/10000\n",
      "3996/3996 - 302s - loss: 0.5748 - accuracy: 0.7638 - val_loss: 0.7427 - val_accuracy: 0.6922\n",
      "Epoch 165/10000\n",
      "3996/3996 - 303s - loss: 0.5730 - accuracy: 0.7661 - val_loss: 0.8040 - val_accuracy: 0.6636\n",
      "Epoch 166/10000\n",
      "3996/3996 - 305s - loss: 0.5719 - accuracy: 0.7658 - val_loss: 0.7598 - val_accuracy: 0.6847\n",
      "Epoch 167/10000\n",
      "3996/3996 - 300s - loss: 0.5714 - accuracy: 0.7655 - val_loss: 0.7764 - val_accuracy: 0.6830\n",
      "Epoch 168/10000\n",
      "3996/3996 - 300s - loss: 0.5674 - accuracy: 0.7674 - val_loss: 0.7636 - val_accuracy: 0.6893\n",
      "Epoch 169/10000\n",
      "3996/3996 - 301s - loss: 0.5669 - accuracy: 0.7675 - val_loss: 0.7469 - val_accuracy: 0.6883\n",
      "Epoch 170/10000\n",
      "3996/3996 - 301s - loss: 0.5678 - accuracy: 0.7668 - val_loss: 0.7383 - val_accuracy: 0.6959\n",
      "Epoch 171/10000\n",
      "3996/3996 - 301s - loss: 0.5676 - accuracy: 0.7667 - val_loss: 0.7811 - val_accuracy: 0.6800\n",
      "Epoch 172/10000\n",
      "3996/3996 - 314s - loss: 0.5619 - accuracy: 0.7689 - val_loss: 0.7729 - val_accuracy: 0.6802\n",
      "Epoch 173/10000\n",
      "3996/3996 - 300s - loss: 0.5640 - accuracy: 0.7675 - val_loss: 0.7647 - val_accuracy: 0.6820\n",
      "Epoch 174/10000\n",
      "3996/3996 - 300s - loss: 0.5641 - accuracy: 0.7686 - val_loss: 0.7511 - val_accuracy: 0.6888\n",
      "Epoch 175/10000\n",
      "3996/3996 - 302s - loss: 0.5584 - accuracy: 0.7723 - val_loss: 0.7693 - val_accuracy: 0.6891\n",
      "Epoch 176/10000\n",
      "3996/3996 - 301s - loss: 0.5558 - accuracy: 0.7738 - val_loss: 0.7415 - val_accuracy: 0.6973\n",
      "Epoch 177/10000\n",
      "3996/3996 - 302s - loss: 0.5614 - accuracy: 0.7688 - val_loss: 0.7588 - val_accuracy: 0.6860\n",
      "Epoch 178/10000\n",
      "3996/3996 - 304s - loss: 0.5570 - accuracy: 0.7725 - val_loss: 0.7639 - val_accuracy: 0.6835\n",
      "Epoch 179/10000\n",
      "3996/3996 - 301s - loss: 0.5544 - accuracy: 0.7737 - val_loss: 0.7625 - val_accuracy: 0.6904\n",
      "Epoch 180/10000\n",
      "3996/3996 - 301s - loss: 0.5513 - accuracy: 0.7743 - val_loss: 0.7522 - val_accuracy: 0.6930\n",
      "Epoch 181/10000\n",
      "3996/3996 - 301s - loss: 0.5537 - accuracy: 0.7728 - val_loss: 0.7709 - val_accuracy: 0.6795\n",
      "Epoch 182/10000\n",
      "3996/3996 - 302s - loss: 0.5534 - accuracy: 0.7730 - val_loss: 0.7766 - val_accuracy: 0.6824\n",
      "Epoch 183/10000\n",
      "3996/3996 - 302s - loss: 0.5483 - accuracy: 0.7750 - val_loss: 0.7395 - val_accuracy: 0.6983\n",
      "Epoch 184/10000\n",
      "3996/3996 - 303s - loss: 0.5479 - accuracy: 0.7757 - val_loss: 0.7293 - val_accuracy: 0.7028\n",
      "Epoch 185/10000\n",
      "3996/3996 - 302s - loss: 0.5534 - accuracy: 0.7713 - val_loss: 0.7630 - val_accuracy: 0.6858\n",
      "Epoch 186/10000\n",
      "3996/3996 - 302s - loss: 0.5459 - accuracy: 0.7761 - val_loss: 0.7857 - val_accuracy: 0.6754\n",
      "Epoch 187/10000\n",
      "3996/3996 - 304s - loss: 0.5440 - accuracy: 0.7777 - val_loss: 0.7370 - val_accuracy: 0.6957\n",
      "Epoch 188/10000\n",
      "3996/3996 - 302s - loss: 0.5427 - accuracy: 0.7786 - val_loss: 0.7328 - val_accuracy: 0.6981\n",
      "Epoch 189/10000\n",
      "3996/3996 - 303s - loss: 0.5437 - accuracy: 0.7786 - val_loss: 0.7496 - val_accuracy: 0.6943\n",
      "Epoch 190/10000\n",
      "3996/3996 - 305s - loss: 0.5488 - accuracy: 0.7744 - val_loss: 0.8212 - val_accuracy: 0.6571\n",
      "Epoch 191/10000\n",
      "3996/3996 - 301s - loss: 0.5437 - accuracy: 0.7785 - val_loss: 0.7432 - val_accuracy: 0.6967\n",
      "Epoch 192/10000\n",
      "3996/3996 - 301s - loss: 0.5422 - accuracy: 0.7792 - val_loss: 0.7561 - val_accuracy: 0.6871\n",
      "Epoch 193/10000\n",
      "3996/3996 - 301s - loss: 0.5445 - accuracy: 0.7774 - val_loss: 0.7719 - val_accuracy: 0.6812\n",
      "Epoch 194/10000\n",
      "3996/3996 - 301s - loss: 0.5415 - accuracy: 0.7792 - val_loss: 0.7571 - val_accuracy: 0.6920\n",
      "Epoch 195/10000\n",
      "3996/3996 - 302s - loss: 0.5420 - accuracy: 0.7792 - val_loss: 0.7616 - val_accuracy: 0.6878\n",
      "Epoch 196/10000\n",
      "3996/3996 - 302s - loss: 0.5370 - accuracy: 0.7812 - val_loss: 0.7793 - val_accuracy: 0.6827\n",
      "Epoch 197/10000\n",
      "3996/3996 - 299s - loss: 0.5360 - accuracy: 0.7818 - val_loss: 0.7812 - val_accuracy: 0.6817\n",
      "Epoch 198/10000\n",
      "3996/3996 - 300s - loss: 0.5404 - accuracy: 0.7807 - val_loss: 0.7861 - val_accuracy: 0.6795\n",
      "Epoch 199/10000\n",
      "3996/3996 - 302s - loss: 0.5337 - accuracy: 0.7814 - val_loss: 0.7762 - val_accuracy: 0.6849\n",
      "Epoch 200/10000\n",
      "3996/3996 - 300s - loss: 0.5328 - accuracy: 0.7828 - val_loss: 0.7544 - val_accuracy: 0.6946\n",
      "Epoch 201/10000\n",
      "3996/3996 - 301s - loss: 0.5363 - accuracy: 0.7816 - val_loss: 0.7555 - val_accuracy: 0.6932\n",
      "Epoch 202/10000\n",
      "3996/3996 - 303s - loss: 0.5277 - accuracy: 0.7852 - val_loss: 0.7491 - val_accuracy: 0.6902\n",
      "Epoch 203/10000\n",
      "3996/3996 - 300s - loss: 0.5290 - accuracy: 0.7844 - val_loss: 0.7398 - val_accuracy: 0.6996\n",
      "Epoch 204/10000\n",
      "3996/3996 - 301s - loss: 0.5271 - accuracy: 0.7854 - val_loss: 0.7325 - val_accuracy: 0.6997\n",
      "Epoch 00204: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2003fe51a60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience = stop_epoch_num, verbose=1)\n",
    "DeepLOB_model_2.fit(X_train, y_train, epochs=10000, batch_size=batch_size, verbose=2, validation_data=(X_test, y_test), callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 40, 1)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 20, 16)\n",
      "(None, 100, 10, 16)\n",
      "(None, 100, 10, 16)\n",
      "(None, 100, 10, 16)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 16)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 32)\n",
      "(None, 100, 1, 96)\n",
      "(None, 100, 96)\n",
      "(None, 64)\n",
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "DeepLOB_model_3_k26 = initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "3996/3996 - 296s - loss: 1.0978 - accuracy: 0.3454 - val_loss: 1.0965 - val_accuracy: 0.3413\n",
      "Epoch 2/120\n",
      "3996/3996 - 298s - loss: 1.0924 - accuracy: 0.3705 - val_loss: 1.0972 - val_accuracy: 0.3639\n",
      "Epoch 3/120\n",
      "3996/3996 - 302s - loss: 1.0727 - accuracy: 0.4107 - val_loss: 1.0507 - val_accuracy: 0.4428\n",
      "Epoch 4/120\n",
      "3996/3996 - 302s - loss: 1.0364 - accuracy: 0.4601 - val_loss: 1.0221 - val_accuracy: 0.4733\n",
      "Epoch 5/120\n",
      "3996/3996 - 301s - loss: 1.0136 - accuracy: 0.4812 - val_loss: 1.0104 - val_accuracy: 0.4813\n",
      "Epoch 6/120\n",
      "3996/3996 - 302s - loss: 1.0052 - accuracy: 0.4898 - val_loss: 1.0027 - val_accuracy: 0.4869\n",
      "Epoch 7/120\n",
      "3996/3996 - 305s - loss: 0.9996 - accuracy: 0.4926 - val_loss: 1.0000 - val_accuracy: 0.4907\n",
      "Epoch 8/120\n",
      "3996/3996 - 302s - loss: 0.9948 - accuracy: 0.4959 - val_loss: 1.0002 - val_accuracy: 0.4890\n",
      "Epoch 9/120\n",
      "3996/3996 - 305s - loss: 0.9902 - accuracy: 0.4990 - val_loss: 0.9917 - val_accuracy: 0.4989\n",
      "Epoch 10/120\n",
      "3996/3996 - 302s - loss: 0.9861 - accuracy: 0.5016 - val_loss: 0.9876 - val_accuracy: 0.5003\n",
      "Epoch 11/120\n",
      "3996/3996 - 302s - loss: 0.9824 - accuracy: 0.5050 - val_loss: 0.9839 - val_accuracy: 0.5027\n",
      "Epoch 12/120\n",
      "3996/3996 - 303s - loss: 0.9784 - accuracy: 0.5104 - val_loss: 1.0057 - val_accuracy: 0.4883\n",
      "Epoch 13/120\n",
      "3996/3996 - 306s - loss: 0.9743 - accuracy: 0.5128 - val_loss: 0.9772 - val_accuracy: 0.5123\n",
      "Epoch 14/120\n",
      "3996/3996 - 305s - loss: 0.9699 - accuracy: 0.5173 - val_loss: 0.9783 - val_accuracy: 0.5099\n",
      "Epoch 15/120\n",
      "3996/3996 - 300s - loss: 0.9652 - accuracy: 0.5224 - val_loss: 0.9751 - val_accuracy: 0.5178\n",
      "Epoch 16/120\n",
      "3996/3996 - 300s - loss: 0.9606 - accuracy: 0.5266 - val_loss: 0.9683 - val_accuracy: 0.5197\n",
      "Epoch 17/120\n",
      "3996/3996 - 301s - loss: 0.9546 - accuracy: 0.5315 - val_loss: 0.9567 - val_accuracy: 0.5316\n",
      "Epoch 18/120\n",
      "3996/3996 - 300s - loss: 0.9493 - accuracy: 0.5352 - val_loss: 0.9622 - val_accuracy: 0.5276\n",
      "Epoch 19/120\n",
      "3996/3996 - 303s - loss: 0.9432 - accuracy: 0.5395 - val_loss: 0.9525 - val_accuracy: 0.5367\n",
      "Epoch 20/120\n",
      "3996/3996 - 301s - loss: 0.9383 - accuracy: 0.5434 - val_loss: 0.9580 - val_accuracy: 0.5288\n",
      "Epoch 21/120\n",
      "3996/3996 - 304s - loss: 0.9314 - accuracy: 0.5492 - val_loss: 0.9447 - val_accuracy: 0.5348\n",
      "Epoch 22/120\n",
      "3996/3996 - 302s - loss: 0.9242 - accuracy: 0.5547 - val_loss: 0.9351 - val_accuracy: 0.5469\n",
      "Epoch 23/120\n",
      "3996/3996 - 302s - loss: 0.9170 - accuracy: 0.5602 - val_loss: 0.9311 - val_accuracy: 0.5504\n",
      "Epoch 24/120\n",
      "3996/3996 - 303s - loss: 0.9093 - accuracy: 0.5656 - val_loss: 0.9155 - val_accuracy: 0.5659\n",
      "Epoch 25/120\n",
      "3996/3996 - 303s - loss: 0.9013 - accuracy: 0.5740 - val_loss: 0.9419 - val_accuracy: 0.5443\n",
      "Epoch 26/120\n",
      "3996/3996 - 304s - loss: 0.8930 - accuracy: 0.5784 - val_loss: 0.9027 - val_accuracy: 0.5717\n",
      "Epoch 27/120\n",
      "3996/3996 - 303s - loss: 0.8857 - accuracy: 0.5836 - val_loss: 0.9086 - val_accuracy: 0.5681\n",
      "Epoch 28/120\n",
      "3996/3996 - 304s - loss: 0.8777 - accuracy: 0.5880 - val_loss: 0.8970 - val_accuracy: 0.5795\n",
      "Epoch 29/120\n",
      "3996/3996 - 303s - loss: 0.8685 - accuracy: 0.5958 - val_loss: 0.8799 - val_accuracy: 0.5903\n",
      "Epoch 30/120\n",
      "3996/3996 - 304s - loss: 0.8577 - accuracy: 0.6032 - val_loss: 0.8976 - val_accuracy: 0.5781\n",
      "Epoch 31/120\n",
      "3996/3996 - 304s - loss: 0.8472 - accuracy: 0.6098 - val_loss: 0.8748 - val_accuracy: 0.5931\n",
      "Epoch 32/120\n",
      "3996/3996 - 303s - loss: 0.8419 - accuracy: 0.6132 - val_loss: 0.8519 - val_accuracy: 0.6092\n",
      "Epoch 33/120\n",
      "3996/3996 - 304s - loss: 0.8328 - accuracy: 0.6175 - val_loss: 0.8586 - val_accuracy: 0.6006\n",
      "Epoch 34/120\n",
      "3996/3996 - 303s - loss: 0.8224 - accuracy: 0.6223 - val_loss: 0.8460 - val_accuracy: 0.6147\n",
      "Epoch 35/120\n",
      "3996/3996 - 306s - loss: 0.8097 - accuracy: 0.6313 - val_loss: 0.8216 - val_accuracy: 0.6258\n",
      "Epoch 36/120\n",
      "3996/3996 - 315s - loss: 0.8032 - accuracy: 0.6347 - val_loss: 0.8282 - val_accuracy: 0.6237\n",
      "Epoch 37/120\n",
      "3996/3996 - 405s - loss: 0.7928 - accuracy: 0.6428 - val_loss: 0.8006 - val_accuracy: 0.6345\n",
      "Epoch 38/120\n",
      "3996/3996 - 317s - loss: 0.7891 - accuracy: 0.6447 - val_loss: 0.8052 - val_accuracy: 0.6377\n",
      "Epoch 39/120\n",
      "3996/3996 - 332s - loss: 0.7735 - accuracy: 0.6524 - val_loss: 0.8009 - val_accuracy: 0.6391\n",
      "Epoch 40/120\n",
      "3996/3996 - 331s - loss: 0.7665 - accuracy: 0.6564 - val_loss: 0.7973 - val_accuracy: 0.6382\n",
      "Epoch 41/120\n",
      "3996/3996 - 327s - loss: 0.7590 - accuracy: 0.6608 - val_loss: 0.7694 - val_accuracy: 0.6555\n",
      "Epoch 42/120\n",
      "3996/3996 - 372s - loss: 0.7454 - accuracy: 0.6692 - val_loss: 0.8091 - val_accuracy: 0.6365\n",
      "Epoch 43/120\n",
      "3996/3996 - 344s - loss: 0.7376 - accuracy: 0.6727 - val_loss: 0.7716 - val_accuracy: 0.6570\n",
      "Epoch 44/120\n",
      "3996/3996 - 343s - loss: 0.7289 - accuracy: 0.6776 - val_loss: 0.7640 - val_accuracy: 0.6569\n",
      "Epoch 45/120\n",
      "3996/3996 - 339s - loss: 0.7184 - accuracy: 0.6834 - val_loss: 0.7369 - val_accuracy: 0.6771\n",
      "Epoch 46/120\n",
      "3996/3996 - 341s - loss: 0.7088 - accuracy: 0.6888 - val_loss: 0.7277 - val_accuracy: 0.6820\n",
      "Epoch 47/120\n",
      "3996/3996 - 351s - loss: 0.7038 - accuracy: 0.6904 - val_loss: 0.7439 - val_accuracy: 0.6738\n",
      "Epoch 48/120\n",
      "3996/3996 - 348s - loss: 0.6938 - accuracy: 0.6974 - val_loss: 0.7606 - val_accuracy: 0.6601\n",
      "Epoch 49/120\n",
      "3996/3996 - 350s - loss: 0.6829 - accuracy: 0.7027 - val_loss: 0.7244 - val_accuracy: 0.6820\n",
      "Epoch 50/120\n",
      "3996/3996 - 345s - loss: 0.6780 - accuracy: 0.7068 - val_loss: 0.7418 - val_accuracy: 0.6758\n",
      "Epoch 51/120\n",
      "3996/3996 - 321s - loss: 0.6645 - accuracy: 0.7142 - val_loss: 0.7371 - val_accuracy: 0.6731\n",
      "Epoch 52/120\n",
      "3996/3996 - 310s - loss: 0.6622 - accuracy: 0.7135 - val_loss: 0.7252 - val_accuracy: 0.6829\n",
      "Epoch 53/120\n",
      "3996/3996 - 349s - loss: 0.6467 - accuracy: 0.7222 - val_loss: 0.7092 - val_accuracy: 0.6956\n",
      "Epoch 54/120\n",
      "3996/3996 - 311s - loss: 0.6432 - accuracy: 0.7231 - val_loss: 0.6741 - val_accuracy: 0.7091\n",
      "Epoch 55/120\n",
      "3996/3996 - 313s - loss: 0.6318 - accuracy: 0.7295 - val_loss: 0.7065 - val_accuracy: 0.6957\n",
      "Epoch 56/120\n",
      "3996/3996 - 329s - loss: 0.6264 - accuracy: 0.7330 - val_loss: 0.6810 - val_accuracy: 0.7065\n",
      "Epoch 57/120\n",
      "3996/3996 - 642s - loss: 0.6186 - accuracy: 0.7368 - val_loss: 0.6660 - val_accuracy: 0.7152\n",
      "Epoch 58/120\n",
      "3996/3996 - 652s - loss: 0.6126 - accuracy: 0.7406 - val_loss: 0.6736 - val_accuracy: 0.7103\n",
      "Epoch 59/120\n",
      "3996/3996 - 658s - loss: 0.6036 - accuracy: 0.7440 - val_loss: 0.6744 - val_accuracy: 0.7095\n",
      "Epoch 60/120\n",
      "3996/3996 - 658s - loss: 0.6052 - accuracy: 0.7445 - val_loss: 0.6394 - val_accuracy: 0.7279\n",
      "Epoch 61/120\n",
      "3996/3996 - 637s - loss: 0.5917 - accuracy: 0.7513 - val_loss: 0.6361 - val_accuracy: 0.7318\n",
      "Epoch 62/120\n",
      "3996/3996 - 682s - loss: 0.5909 - accuracy: 0.7502 - val_loss: 0.6594 - val_accuracy: 0.7191\n",
      "Epoch 63/120\n",
      "3996/3996 - 421s - loss: 0.5779 - accuracy: 0.7573 - val_loss: 0.6301 - val_accuracy: 0.7335\n",
      "Epoch 64/120\n",
      "3996/3996 - 315s - loss: 0.5794 - accuracy: 0.7554 - val_loss: 0.6501 - val_accuracy: 0.7251\n",
      "Epoch 65/120\n",
      "3996/3996 - 331s - loss: 0.5703 - accuracy: 0.7615 - val_loss: 0.6276 - val_accuracy: 0.7348\n",
      "Epoch 66/120\n",
      "3996/3996 - 332s - loss: 0.5634 - accuracy: 0.7650 - val_loss: 0.6153 - val_accuracy: 0.7437\n",
      "Epoch 67/120\n",
      "3996/3996 - 327s - loss: 0.5575 - accuracy: 0.7682 - val_loss: 0.6345 - val_accuracy: 0.7339\n",
      "Epoch 68/120\n",
      "3996/3996 - 333s - loss: 0.5523 - accuracy: 0.7706 - val_loss: 0.6229 - val_accuracy: 0.7401\n",
      "Epoch 69/120\n",
      "3996/3996 - 332s - loss: 0.5432 - accuracy: 0.7748 - val_loss: 0.6362 - val_accuracy: 0.7330\n",
      "Epoch 70/120\n",
      "3996/3996 - 328s - loss: 0.5373 - accuracy: 0.7772 - val_loss: 0.6184 - val_accuracy: 0.7409\n",
      "Epoch 71/120\n",
      "3996/3996 - 332s - loss: 0.5334 - accuracy: 0.7783 - val_loss: 0.6046 - val_accuracy: 0.7487\n",
      "Epoch 72/120\n",
      "3996/3996 - 332s - loss: 0.5328 - accuracy: 0.7791 - val_loss: 0.6041 - val_accuracy: 0.7496\n",
      "Epoch 73/120\n",
      "3996/3996 - 331s - loss: 0.5318 - accuracy: 0.7806 - val_loss: 0.6132 - val_accuracy: 0.7462\n",
      "Epoch 74/120\n",
      "3996/3996 - 334s - loss: 0.5185 - accuracy: 0.7855 - val_loss: 0.6307 - val_accuracy: 0.7379\n",
      "Epoch 75/120\n",
      "3996/3996 - 329s - loss: 0.5186 - accuracy: 0.7863 - val_loss: 0.6500 - val_accuracy: 0.7319\n",
      "Epoch 76/120\n",
      "3996/3996 - 479s - loss: 0.5126 - accuracy: 0.7891 - val_loss: 0.6083 - val_accuracy: 0.7465\n",
      "Epoch 77/120\n",
      "3996/3996 - 612s - loss: 0.5079 - accuracy: 0.7906 - val_loss: 0.5543 - val_accuracy: 0.7716\n",
      "Epoch 78/120\n",
      "3996/3996 - 318s - loss: 0.5008 - accuracy: 0.7946 - val_loss: 0.6200 - val_accuracy: 0.7423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/120\n",
      "3996/3996 - 318s - loss: 0.4981 - accuracy: 0.7964 - val_loss: 0.5825 - val_accuracy: 0.7582\n",
      "Epoch 80/120\n",
      "3996/3996 - 322s - loss: 0.4984 - accuracy: 0.7969 - val_loss: 0.5771 - val_accuracy: 0.7599\n",
      "Epoch 81/120\n",
      "3996/3996 - 325s - loss: 0.4904 - accuracy: 0.8009 - val_loss: 0.6058 - val_accuracy: 0.7531\n",
      "Epoch 82/120\n",
      "3996/3996 - 326s - loss: 0.4955 - accuracy: 0.7987 - val_loss: 0.5614 - val_accuracy: 0.7717\n",
      "Epoch 83/120\n",
      "3996/3996 - 325s - loss: 0.4877 - accuracy: 0.8012 - val_loss: 0.6697 - val_accuracy: 0.7244\n",
      "Epoch 84/120\n",
      "3996/3996 - 320s - loss: 0.4864 - accuracy: 0.8005 - val_loss: 0.5677 - val_accuracy: 0.7680\n",
      "Epoch 85/120\n",
      "3996/3996 - 328s - loss: 0.4782 - accuracy: 0.8041 - val_loss: 0.6067 - val_accuracy: 0.7538\n",
      "Epoch 86/120\n",
      "3996/3996 - 325s - loss: 0.4744 - accuracy: 0.8076 - val_loss: 0.5478 - val_accuracy: 0.7763\n",
      "Epoch 87/120\n",
      "3996/3996 - 326s - loss: 0.4703 - accuracy: 0.8094 - val_loss: 0.5855 - val_accuracy: 0.7600\n",
      "Epoch 88/120\n",
      "3996/3996 - 324s - loss: 0.4705 - accuracy: 0.8084 - val_loss: 0.5504 - val_accuracy: 0.7784\n",
      "Epoch 89/120\n",
      "3996/3996 - 328s - loss: 0.4700 - accuracy: 0.8099 - val_loss: 0.5618 - val_accuracy: 0.7736\n",
      "Epoch 90/120\n",
      "3996/3996 - 322s - loss: 0.4620 - accuracy: 0.8126 - val_loss: 0.5932 - val_accuracy: 0.7603\n",
      "Epoch 91/120\n",
      "3996/3996 - 325s - loss: 0.4609 - accuracy: 0.8145 - val_loss: 0.5642 - val_accuracy: 0.7711\n",
      "Epoch 92/120\n",
      "3996/3996 - 328s - loss: 0.4582 - accuracy: 0.8149 - val_loss: 0.5199 - val_accuracy: 0.7922\n",
      "Epoch 93/120\n",
      "3996/3996 - 324s - loss: 0.4524 - accuracy: 0.8173 - val_loss: 0.5380 - val_accuracy: 0.7832\n",
      "Epoch 94/120\n",
      "3996/3996 - 316s - loss: 0.4563 - accuracy: 0.8151 - val_loss: 0.5418 - val_accuracy: 0.7820\n",
      "Epoch 95/120\n",
      "3996/3996 - 431s - loss: 0.4504 - accuracy: 0.8185 - val_loss: 0.5554 - val_accuracy: 0.7758\n",
      "Epoch 96/120\n",
      "3996/3996 - 308s - loss: 0.4467 - accuracy: 0.8201 - val_loss: 0.5491 - val_accuracy: 0.7772\n",
      "Epoch 97/120\n",
      "3996/3996 - 313s - loss: 0.4482 - accuracy: 0.8175 - val_loss: 0.5690 - val_accuracy: 0.7677\n",
      "Epoch 98/120\n",
      "3996/3996 - 314s - loss: 0.4395 - accuracy: 0.8225 - val_loss: 0.5470 - val_accuracy: 0.7802\n",
      "Epoch 99/120\n",
      "3996/3996 - 336s - loss: 0.4401 - accuracy: 0.8232 - val_loss: 0.6160 - val_accuracy: 0.7514\n",
      "Epoch 100/120\n",
      "3996/3996 - 339s - loss: 0.4378 - accuracy: 0.8234 - val_loss: 0.5371 - val_accuracy: 0.7836\n",
      "Epoch 101/120\n",
      "3996/3996 - 326s - loss: 0.4332 - accuracy: 0.8257 - val_loss: 0.5337 - val_accuracy: 0.7858\n",
      "Epoch 102/120\n",
      "3996/3996 - 329s - loss: 0.4329 - accuracy: 0.8248 - val_loss: 0.5228 - val_accuracy: 0.7943\n",
      "Epoch 103/120\n",
      "3996/3996 - 329s - loss: 0.4292 - accuracy: 0.8276 - val_loss: 0.5899 - val_accuracy: 0.7651\n",
      "Epoch 104/120\n",
      "3996/3996 - 330s - loss: 0.4271 - accuracy: 0.8286 - val_loss: 0.5310 - val_accuracy: 0.7872\n",
      "Epoch 105/120\n",
      "3996/3996 - 325s - loss: 0.4229 - accuracy: 0.8297 - val_loss: 0.5513 - val_accuracy: 0.7737\n",
      "Epoch 106/120\n",
      "3996/3996 - 493s - loss: 0.4250 - accuracy: 0.8280 - val_loss: 0.4998 - val_accuracy: 0.8027\n",
      "Epoch 107/120\n",
      "3996/3996 - 699s - loss: 0.4182 - accuracy: 0.8329 - val_loss: 0.5186 - val_accuracy: 0.7943\n",
      "Epoch 108/120\n",
      "3996/3996 - 704s - loss: 0.4222 - accuracy: 0.8299 - val_loss: 0.5504 - val_accuracy: 0.7797\n",
      "Epoch 109/120\n",
      "3996/3996 - 681s - loss: 0.4157 - accuracy: 0.8330 - val_loss: 0.5093 - val_accuracy: 0.7981\n",
      "Epoch 110/120\n",
      "3996/3996 - 600s - loss: 0.4221 - accuracy: 0.8316 - val_loss: 0.5086 - val_accuracy: 0.7994\n",
      "Epoch 111/120\n",
      "3996/3996 - 311s - loss: 0.4219 - accuracy: 0.8307 - val_loss: 0.5979 - val_accuracy: 0.7594\n",
      "Epoch 112/120\n",
      "3996/3996 - 329s - loss: 0.4133 - accuracy: 0.8340 - val_loss: 0.5461 - val_accuracy: 0.7846\n",
      "Epoch 113/120\n",
      "3996/3996 - 348s - loss: 0.4067 - accuracy: 0.8376 - val_loss: 0.4962 - val_accuracy: 0.8050\n",
      "Epoch 114/120\n",
      "3996/3996 - 356s - loss: 0.4070 - accuracy: 0.8374 - val_loss: 0.5603 - val_accuracy: 0.7788\n",
      "Epoch 115/120\n",
      "3996/3996 - 335s - loss: 0.4007 - accuracy: 0.8391 - val_loss: 0.5273 - val_accuracy: 0.7940\n",
      "Epoch 116/120\n",
      "3996/3996 - 329s - loss: 0.4062 - accuracy: 0.8379 - val_loss: 0.5494 - val_accuracy: 0.7832\n",
      "Epoch 117/120\n",
      "3996/3996 - 334s - loss: 0.4068 - accuracy: 0.8379 - val_loss: 0.5390 - val_accuracy: 0.7883\n",
      "Epoch 118/120\n",
      "3996/3996 - 333s - loss: 0.4023 - accuracy: 0.8398 - val_loss: 0.5197 - val_accuracy: 0.7940\n",
      "Epoch 119/120\n",
      "3996/3996 - 326s - loss: 0.3970 - accuracy: 0.8419 - val_loss: 0.5347 - val_accuracy: 0.7884\n",
      "Epoch 120/120\n",
      "3996/3996 - 331s - loss: 0.3996 - accuracy: 0.8405 - val_loss: 0.5514 - val_accuracy: 0.7754\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience = stop_epoch_num, verbose=1)\n",
    "history_k26 = DeepLOB_model_3_k26.fit(X_train3, y_train3, epochs=num_epoch, batch_size=batch_size, verbose=2, validation_data=(X_test3, y_test3), callbacks = [es])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
